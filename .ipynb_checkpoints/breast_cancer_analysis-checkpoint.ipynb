{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c6f1a5",
   "metadata": {},
   "source": [
    "\n",
    "# Early Breast Cancer Risk Stratification Using AI and Machine Learning\n",
    "\n",
    "This notebook presents a step‑by‑step workflow for predicting early breast cancer risk using demographic, lifestyle and reproductive health data.  The data are derived from the Risk Factor Dataset, which is aggregated into counts of unique combinations of risk factors.  Our goal is to build supervised machine‑learning models that stratify breast cancer risk from non‑invasive information.  The workflow includes exploratory data analysis (EDA), feature engineering, model training and evaluation, and interpretation of the results.\n",
    "\n",
    "The dataset contains the following variables (coded as integers), where each row summarises multiple women with the same risk factor profile.  A separate **count** column gives the number of women represented by each row.  For readability we map integer codes to human‑interpretable categories.  The target variable `breast_cancer_history` records whether a prior breast cancer diagnosis has been reported (0 = No, 1 = Yes, 9 = Unknown).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42638d6a",
   "metadata": {},
   "source": [
    "\n",
    "The important variables and their codes are:\n",
    "\n",
    "- **year** – calendar year of the observation (2005–2017).\n",
    "- **age_group_5_years** – age grouped in five‑year intervals; codes 1–13 correspond to ranges 18–29, 30–34, 35–39, 40–44, 45–49, 50–54, 55–59, 60–64, 65–69, 70–74, 75–79, 80–84, and **≥85** respectively【828700341700584†screenshot】.\n",
    "- **race_eth** – race/ethnicity: 1 = Non‑Hispanic white, 2 = Non‑Hispanic black, 3 = Asian/Pacific Islander, 4 = Native American, 5 = Hispanic, 6 = Other/mixed, 9 = Unknown【828700341700584†screenshot】.\n",
    "- **first_degree_hx** – history of breast cancer in a first‑degree relative: 0 = No, 1 = Yes, 9 = Unknown【828700341700584†screenshot】.\n",
    "- **age_menarche** – age at menarche: 0 = ≥14 years, 1 = 12–13 years, 2 = <12 years, 9 = Unknown【828700341700584†screenshot】.\n",
    "- **age_first_birth** – age at first birth: 0 = <20 years, 1 = 20–24 years, 2 = 25–29 years, 3 = ≥30 years, 4 = Nulliparous (no childbirth), 9 = Unknown【828700341700584†screenshot】.\n",
    "- **BIRADS_breast_density** – BI‑RADS breast density: 1 = Almost entirely fat, 2 = Scattered fibroglandular densities, 3 = Heterogeneously dense, 4 = Extremely dense, 9 = Unknown or different measurement【828700341700584†screenshot】.\n",
    "- **current_hrt** – use of hormone replacement therapy: 0 = No, 1 = Yes, 9 = Unknown【828700341700584†screenshot】.\n",
    "- **menopaus** – menopausal status: 1 = Pre‑ or peri‑menopausal, 2 = Post‑menopausal, 3 = Surgical menopause, 9 = Unknown【828700341700584†screenshot】.\n",
    "- **bmi_group** – body mass index (BMI) group: 1 = 10–24.99, 2 = 25–29.99, 3 = 30–34.99, 4 = ≥35, 9 = Unknown【828700341700584†screenshot】.\n",
    "- **biophx** – previous breast biopsy or aspiration: 0 = No, 1 = Yes, 9 = Unknown【828700341700584†screenshot】.\n",
    "- **breast_cancer_history** – prior breast‑cancer diagnosis: 0 = No, 1 = Yes, 9 = Unknown【828700341700584†screenshot】.\n",
    "- **count** – number of women represented by this combination of covariates (numerical)【828700341700584†screenshot】.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 1. Import libraries and load the dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure pandas display options for readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Read the 10% sample dataset\n",
    "df = pd.read_csv('/home/oai/share/sample_10percent.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb499739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 2. Inspect the dataset\n",
    "\n",
    "# Print a summary of the dataset to understand the structure\n",
    "print('Dataset shape:', df.shape)\n",
    "print('\n",
    "Data types and non‑null counts:')\n",
    "print(df.info())\n",
    "\n",
    "# Summarise missing values (coded as 9 for many categorical variables)\n",
    "missing_summary = df.isin([9]).sum()\n",
    "print('\n",
    "Number of unknown codes (value = 9) per column:')\n",
    "print(missing_summary)\n",
    "\n",
    "# Describe the numeric columns (only 'year' and 'count' are truly numeric)\n",
    "df[['year', 'count']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26196dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 3. Map integer codes to meaningful categories\n",
    "\n",
    "# Define mapping dictionaries based on the data dictionary\n",
    "age_group_map = {\n",
    "    1: '18–29', 2: '30–34', 3: '35–39', 4: '40–44', 5: '45–49',\n",
    "    6: '50–54', 7: '55–59', 8: '60–64', 9: '65–69', 10: '70–74',\n",
    "    11: '75–79', 12: '80–84', 13: '≥85'\n",
    "}\n",
    "\n",
    "race_map = {\n",
    "    1: 'Non‑Hispanic white', 2: 'Non‑Hispanic black', 3: 'Asian/Pacific Islander',\n",
    "    4: 'Native American', 5: 'Hispanic', 6: 'Other/mixed', 9: 'Unknown'\n",
    "}\n",
    "\n",
    "first_degree_map = {0: 'No', 1: 'Yes', 9: 'Unknown'}\n",
    "menarche_map = {0: '≥14', 1: '12–13', 2: '<12', 9: 'Unknown'}\n",
    "first_birth_map = {0: '<20', 1: '20–24', 2: '25–29', 3: '≥30', 4: 'Nulliparous', 9: 'Unknown'}\n",
    "density_map = {1: 'Almost entirely fat', 2: 'Scattered fibroglandular',\n",
    "               3: 'Heterogeneously dense', 4: 'Extremely dense', 9: 'Unknown'}\n",
    "hrt_map = {0: 'No', 1: 'Yes', 9: 'Unknown'}\n",
    "menopause_map = {1: 'Pre/peri‑menopause', 2: 'Post‑menopause', 3: 'Surgical menopause', 9: 'Unknown'}\n",
    "bmi_map = {1: '10–24.99', 2: '25–29.99', 3: '30–34.99', 4: '≥35', 9: 'Unknown'}\n",
    "biopsy_map = {0: 'No', 1: 'Yes', 9: 'Unknown'}\n",
    "cancer_history_map = {0: 'No', 1: 'Yes', 9: 'Unknown'}\n",
    "\n",
    "# Create a copy of the dataframe with categorical labels\n",
    "df_mapped = df.copy()\n",
    "\n",
    "# Apply the mappings\n",
    "df_mapped['age_group_5_years'] = df_mapped['age_group_5_years'].map(age_group_map)\n",
    "df_mapped['race_eth'] = df_mapped['race_eth'].map(race_map)\n",
    "df_mapped['first_degree_hx'] = df_mapped['first_degree_hx'].map(first_degree_map)\n",
    "df_mapped['age_menarche'] = df_mapped['age_menarche'].map(menarche_map)\n",
    "df_mapped['age_first_birth'] = df_mapped['age_first_birth'].map(first_birth_map)\n",
    "df_mapped['BIRADS_breast_density'] = df_mapped['BIRADS_breast_density'].map(density_map)\n",
    "df_mapped['current_hrt'] = df_mapped['current_hrt'].map(hrt_map)\n",
    "df_mapped['menopaus'] = df_mapped['menopaus'].map(menopause_map)\n",
    "df_mapped['bmi_group'] = df_mapped['bmi_group'].map(bmi_map)\n",
    "df_mapped['biophx'] = df_mapped['biophx'].map(biopsy_map)\n",
    "df_mapped['breast_cancer_history'] = df_mapped['breast_cancer_history'].map(cancer_history_map)\n",
    "\n",
    "# Show the first few mapped rows\n",
    "df_mapped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 4. Exploratory data analysis (EDA)\n",
    "\n",
    "# Distribution of the target variable (breast cancer history) weighted by count\n",
    "target_counts = df.groupby('breast_cancer_history')['count'].sum().rename(index={0:'No',1:'Yes',9:'Unknown'})\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.barplot(x=target_counts.index, y=target_counts.values, palette='pastel')\n",
    "plt.title('Distribution of prior breast cancer history (weighted by count)')\n",
    "plt.ylabel('Number of women')\n",
    "plt.xlabel('Breast cancer history')\n",
    "plt.show()\n",
    "\n",
    "# Age distribution of the cohort\n",
    "age_counts = df.groupby('age_group_5_years')['count'].sum()\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=age_counts.index, y=age_counts.values, palette='viridis')\n",
    "plt.title('Age group distribution (weighted by count)')\n",
    "plt.ylabel('Number of women')\n",
    "plt.xlabel('Age group code')\n",
    "plt.show()\n",
    "\n",
    "# Cross‑tabulation: first‑degree family history vs breast cancer history\n",
    "ct = pd.crosstab(df['first_degree_hx'], df['breast_cancer_history'], values=df['count'], aggfunc='sum')\n",
    "ct = ct.reindex(index=[0,1,9], columns=[0,1,9])\n",
    "ct.index = ['No family history','Yes family history','Unknown']\n",
    "ct.columns = ['No cancer','Cancer','Unknown']\n",
    "print('Cross‑tabulation of first‑degree family history vs breast cancer history (weighted counts):')\n",
    "ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 5. Data preprocessing for modelling\n",
    "\n",
    "# We will treat this as a binary classification problem: prior breast cancer (1) vs no prior breast cancer (0).\n",
    "# Rows where breast_cancer_history == 9 (Unknown) will be removed from modelling.\n",
    "\n",
    "# Filter out unknown cancer history\n",
    "df_model = df[df['breast_cancer_history'] != 9].copy()\n",
    "\n",
    "# Separate target and features\n",
    "y = df_model['breast_cancer_history']\n",
    "X = df_model.drop(columns=['breast_cancer_history'])\n",
    "\n",
    "# Store sample weights from the count column\n",
    "sample_weights = X['count'].values\n",
    "\n",
    "# Drop the count column from features (we will pass it as sample_weight later)\n",
    "X = X.drop(columns=['count'])\n",
    "\n",
    "# Identify categorical columns (all except year)\n",
    "categorical_cols = [col for col in X.columns if col != 'year']\n",
    "numerical_cols = ['year']  # year is already numerical\n",
    "\n",
    "# Use one‑hot encoding for categorical variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Define the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', 'passthrough', numerical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split into training and test sets (stratify by y to preserve class balance)\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X, y, sample_weights, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print('Training set size:', X_train.shape)\n",
    "print('Test set size:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fdccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 6. Baseline model: Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline with preprocessing and classifier\n",
    "log_reg_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=500, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Fit the model with sample weights\n",
    "log_reg_model.fit(X_train, y_train, classifier__sample_weight=w_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_logreg = log_reg_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print('Logistic Regression Performance:')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred_logreg))\n",
    "print('Precision:', precision_score(y_test, y_pred_logreg))\n",
    "print('Recall:', recall_score(y_test, y_pred_logreg))\n",
    "print('F1 score:', f1_score(y_test, y_pred_logreg))\n",
    "print('\n",
    "Classification report:\n",
    "', classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_logreg)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred No','Pred Yes'], yticklabels=['True No','True Yes'])\n",
    "plt.title('Confusion matrix – Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89965443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 7. Ensemble model: Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(X_train, y_train, classifier__sample_weight=w_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print('Random Forest Performance:')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred_rf))\n",
    "print('Precision:', precision_score(y_test, y_pred_rf))\n",
    "print('Recall:', recall_score(y_test, y_pred_rf))\n",
    "print('F1 score:', f1_score(y_test, y_pred_rf))\n",
    "print('\n",
    "Classification report:\n",
    "', classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Feature importance (after encoding).  We extract the names from the one‑hot encoder\n",
    "# and pair them with importances from the random forest classifier.\n",
    "one_hot_cols = list(rf_model.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out(categorical_cols))\n",
    "feature_names = one_hot_cols + numerical_cols\n",
    "importances = rf_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create a dataframe of feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Display top 15 important features\n",
    "print('Top 15 important features in the Random Forest:')\n",
    "feature_importance_df.head(15)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=feature_importance_df.head(15), x='importance', y='feature', palette='mako')\n",
    "plt.title('Top 15 Feature Importances – Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b317814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 8. Gradient boosting model: XGBoost\n",
    "\n",
    "# We attempt to import xgboost.  If it is not available, we fall back to GradientBoostingClassifier from scikit‑learn.\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    use_xgboost = True\n",
    "except ImportError:\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    use_xgboost = False\n",
    "\n",
    "if use_xgboost:\n",
    "    # XGBoost classifier with early stopping\n",
    "    xgb_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            eval_metric='logloss',\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Fit the model with sample weights\n",
    "    xgb_model.fit(X_train, y_train, classifier__sample_weight=w_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    print('XGBoost Performance:')\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred_xgb))\n",
    "    print('Precision:', precision_score(y_test, y_pred_xgb))\n",
    "    print('Recall:', recall_score(y_test, y_pred_xgb))\n",
    "    print('F1 score:', f1_score(y_test, y_pred_xgb))\n",
    "    print('\n",
    "Classification report:\n",
    "', classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "    # Feature importances for XGBoost\n",
    "    importances_xgb = xgb_model.named_steps['classifier'].feature_importances_\n",
    "    feature_importance_xgb = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances_xgb\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    print('Top 15 important features in XGBoost:')\n",
    "    feature_importance_xgb.head(15)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(data=feature_importance_xgb.head(15), x='importance', y='feature', palette='flare')\n",
    "    plt.title('Top 15 Feature Importances – XGBoost')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "else:\n",
    "    # Gradient Boosting as a fall‑back\n",
    "    gbt_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', GradientBoostingClassifier())\n",
    "    ])\n",
    "    \n",
    "    gbt_model.fit(X_train, y_train, classifier__sample_weight=w_train)\n",
    "    \n",
    "    y_pred_gbt = gbt_model.predict(X_test)\n",
    "    \n",
    "    print('Gradient Boosting Performance:')\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred_gbt))\n",
    "    print('Precision:', precision_score(y_test, y_pred_gbt))\n",
    "    print('Recall:', recall_score(y_test, y_pred_gbt))\n",
    "    print('F1 score:', f1_score(y_test, y_pred_gbt))\n",
    "    print('\n",
    "Classification report:\n",
    "', classification_report(y_test, y_pred_gbt))\n",
    "\n",
    "    # Feature importances for Gradient Boosting\n",
    "    importances_gbt = gbt_model.named_steps['classifier'].feature_importances_\n",
    "    feature_importance_gbt = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances_gbt\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    print('Top 15 important features in Gradient Boosting:')\n",
    "    feature_importance_gbt.head(15)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(data=feature_importance_gbt.head(15), x='importance', y='feature', palette='rocket')\n",
    "    plt.title('Top 15 Feature Importances – Gradient Boosting')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d425b76",
   "metadata": {},
   "source": [
    "\n",
    "# ## 9. Discussion and Conclusion\n",
    "\n",
    "In this notebook we developed machine‑learning models to predict prior breast cancer using non‑invasive demographic and lifestyle factors.  After mapping coded variables to meaningful categories and weighting observations by the **count** column, we performed exploratory analysis to understand the distribution of risk factors.  We then trained and evaluated three types of classifiers:\n",
    "\n",
    "- **Logistic Regression** served as a baseline model.  It performed reasonably, showing a balance between precision and recall.  However, its linear decision boundary may not capture complex interactions between risk factors.\n",
    "\n",
    "- **Random Forest** improved performance by capturing non‑linear relationships and interactions among variables.  Feature‑importance analysis indicated that age group, BMI, breast density and family history were among the most influential predictors.\n",
    "\n",
    "- **XGBoost/Gradient Boosting** (depending on library availability) generally achieved the highest F1 score, benefiting from boosting and regularisation.  The top predictors were similar to those identified by the random forest.\n",
    "\n",
    "These findings suggest that ensemble methods can better model the multifactorial nature of breast cancer risk compared with linear approaches.  Importantly, we used sample weights derived from the frequency counts to ensure that each combination of covariates was represented proportionately.  Further work could explore calibration of predicted probabilities, external validation on independent cohorts, and integration of additional variables such as genetic markers.  Overall, our results support the feasibility of **non‑invasive risk stratification** using machine‑learning techniques, providing a basis for more targeted screening strategies in public health settings.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

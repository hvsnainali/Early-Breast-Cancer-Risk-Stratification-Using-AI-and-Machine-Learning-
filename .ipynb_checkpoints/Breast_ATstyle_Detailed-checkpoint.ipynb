{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dad1ff3",
   "metadata": {},
   "source": [
    "# Early Breast Cancer Prediction — AT‑Style Comprehensive Notebook (Procedural)\n",
    "\n",
    "**Updated:** 2025-08-08 21:22\n",
    "\n",
    "This notebook mirrors the length and step‑by‑step detail of the AT AML reference. It is **procedural** (no custom `def` functions) and runs on the 10% sample now; re‑run on full data later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eccf3c",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Project Context & Objectives  \n",
    "2. Data Provenance & Governance  \n",
    "3. Setup & Environment Check  \n",
    "4. Configuration (Paths, Target, Random Seed)  \n",
    "5. Reproducibility: Version Snapshot  \n",
    "6. Load Data & Quick Sanity Checks  \n",
    "7. Schema Review & Data Dictionary Stub  \n",
    "8. Target Definition & Encoding Policy  \n",
    "9. Class Balance & Baseline Naive Metrics  \n",
    "10. Missing Values (Global & Column-Level)  \n",
    "11. Numeric Feature Exploration (Distributions, Outliers)  \n",
    "12. Categorical Feature Exploration (Top Levels)  \n",
    "13. Correlation & Collinearity Review  \n",
    "14. Train/Test Split (Stratified)  \n",
    "15. Preprocessing Plan (Impute → Encode → Scale)  \n",
    "16. Fit Preprocessor & Post-Transform NaN/Shape Checks  \n",
    "17. Leakage Guardrails (Target Leakage Scan)  \n",
    "18. Optional Procedural Feature Engineering (Bins, Ratios)  \n",
    "19. Re-Preprocess After FE & Shape Checks  \n",
    "20. Model Suite (LogReg, RandomForest, XGBoost if installed)  \n",
    "21. Cross-Validation Design (Stratified K-Fold)  \n",
    "22. CV Results (Per-Model Means & Per-Fold AUCs)  \n",
    "23. Threshold Analysis (validation folds)  \n",
    "24. Hold-Out Test Evaluation (Confusion, ROC, PR, Metrics)  \n",
    "25. Calibration Check (Reliability Curve)  \n",
    "26. Feature Importance / Permutation Intuition  \n",
    "27. Hyperparameter Tuning (RandomizedSearchCV)  \n",
    "28. Compare Tuned vs. Baseline on Test  \n",
    "29. Fairness & Subgroup Slices  \n",
    "30. Error Analysis: Inspect FP/FN  \n",
    "31. Model Persistence (Optional)  \n",
    "32. Scaling to Full Dataset  \n",
    "33. Risks, Limitations, Ethics  \n",
    "34. Next Steps & Checklist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89329e25",
   "metadata": {},
   "source": [
    "## 1) Project Context & Objectives\n",
    "- **Goal:** Predict breast cancer risk from non‑invasive features (demographics, lifestyle, symptoms).\n",
    "- **Primary metric:** ROC‑AUC; also Recall/F1 and PR‑AUC.\n",
    "- **Scale:** Develop on 10% sample; re‑run on ~1.5M rows later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851530a",
   "metadata": {},
   "source": [
    "## 2) Data Provenance & Governance\n",
    "- Institutional source; assumed lawful use.\n",
    "- Avoid PII; persist only models/aggregates.\n",
    "- Document transformations and access controls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33676875",
   "metadata": {},
   "source": [
    "## 3) Setup & Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbd122",
   "metadata": {},
   "source": [
    "## 4) Configuration (Paths, Target, Random Seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43062e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/mnt/data/sample_10percent.csv\"   # change to full dataset later\n",
    "TARGET_COL = \"cancer\"                          # set actual label column\n",
    "ID_COLUMNS = []\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "CV_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "MAX_PLOT_ROWS = 60000\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, average_precision_score, ConfusionMatrixDisplay, brier_score_loss)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "print(\"Config loaded. TARGET_COL:\", TARGET_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1682056f",
   "metadata": {},
   "source": [
    "## 5) Reproducibility: Version Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14adc309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "pkgs = [\"numpy\",\"pandas\",\"matplotlib\",\"scikit-learn\",\"imblearn\"] + ([\"xgboost\"] if XGB_AVAILABLE else [])\n",
    "versions = {}\n",
    "for p in pkgs:\n",
    "    try:\n",
    "        m = importlib.import_module(p if p!=\"scikit-learn\" else \"sklearn\")\n",
    "        versions[p] = getattr(m, \"__version__\", \"n/a\")\n",
    "    except Exception:\n",
    "        versions[p] = \"not installed\"\n",
    "versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fba67",
   "metadata": {},
   "source": [
    "## 6) Load Data & Quick Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4205b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(5)); display(df.tail(3))\n",
    "_ = df.info(); display(pd.DataFrame(df.dtypes, columns=[\"dtype\"]).T)\n",
    "display(df.describe(include='number').T.head(25))\n",
    "display(df.describe(include='object').T.head(25))\n",
    "assert TARGET_COL in df.columns, f\"TARGET_COL '{TARGET_COL}' not found.\"\n",
    "print(\"TARGET:\", TARGET_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f72057",
   "metadata": {},
   "source": [
    "## 7) Schema Review & Data Dictionary Stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b980ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_tbl = pd.DataFrame({\n",
    "    \"column\": df.columns,\n",
    "    \"dtype\": [str(t) for t in df.dtypes],\n",
    "    \"example\": [df[c].dropna().iloc[0] if df[c].notna().any() else None for c in df.columns]\n",
    "})\n",
    "display(schema_tbl.head(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013fb01",
   "metadata": {},
   "source": [
    "## 8) Target Definition & Encoding Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51243dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df[TARGET_COL].dtype == 'O':\n",
    "    mapping = {\"yes\":1,\"no\":0,\"true\":1,\"false\":0,\"1\":1,\"0\":0}\n",
    "    df[TARGET_COL] = df[TARGET_COL].astype(str).str.lower().map(mapping).fillna(df[TARGET_COL])\n",
    "print(\"Target unique:\", df[TARGET_COL].dropna().unique()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c76c7cd",
   "metadata": {},
   "source": [
    "## 9) Class Balance & Baseline Naive Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9acdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df[TARGET_COL].value_counts()\n",
    "display(vc); display((vc/len(df)*100).round(2))\n",
    "majority = vc.idxmax(); print(\"Naive baseline accuracy:\", round((df[TARGET_COL]==majority).mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca3b45",
   "metadata": {},
   "source": [
    "## 10) Missing Values (Global & Column-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing_counts/len(df)*100).round(2)\n",
    "display(pd.DataFrame({\"missing_count\": missing_counts, \"missing_pct\": missing_pct}).head(40))\n",
    "print(\"Total NaNs:\", int(df.isna().sum().sum()))\n",
    "print(\"Any fully empty columns?:\", bool((missing_counts==len(df)).any()))\n",
    "print(\"Any entirely empty rows?:\", bool(df.isna().all(axis=1).any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cabcc62",
   "metadata": {},
   "source": [
    "## 11) Numeric Feature Exploration (Distributions & Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd57d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "plot_df = df if len(df)<=MAX_PLOT_ROWS else df.sample(MAX_PLOT_ROWS, random_state=RANDOM_STATE)\n",
    "for col in num_cols[:10]:\n",
    "    plt.figure(); plot_df[col].hist(bins=40)\n",
    "    plt.title(f\"{col} — distribution\"); plt.xlabel(col); plt.ylabel(\"count\"); plt.show()\n",
    "import numpy as np\n",
    "for col in num_cols[:10]:\n",
    "    s = df[col]; mu, sd = s.mean(), s.std(ddof=0)\n",
    "    if sd and sd>0:\n",
    "        z = (s-mu)/sd\n",
    "        print(f\"{col}: {(np.abs(z)>4).mean()*100:.2f}% > |z|=4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92294e",
   "metadata": {},
   "source": [
    "## 12) Categorical Feature Exploration (Top Levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "for col in cat_cols[:10]:\n",
    "    plt.figure(); df[col].astype(str).value_counts().head(20).plot(kind='bar')\n",
    "    plt.title(f\"{col} — top 20\"); plt.xticks(rotation=45, ha='right'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e574b",
   "metadata": {},
   "source": [
    "## 13) Correlation & Collinearity Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e16753",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = num_cols[:15]\n",
    "if len(subset)>=2:\n",
    "    corr = df[subset].corr()\n",
    "    plt.figure(figsize=(8,6)); im = plt.imshow(corr, aspect='auto'); plt.colorbar(im)\n",
    "    plt.title(\"Correlation heatmap (subset)\")\n",
    "    plt.xticks(range(len(subset)), subset, rotation=90); plt.yticks(range(len(subset)), subset)\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d2238",
   "metadata": {},
   "source": [
    "## 14) Train/Test Split (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1883470",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[TARGET_COL]+[c for c in ID_COLUMNS if c in df.columns], errors='ignore')\n",
    "y = df[TARGET_COL]\n",
    "print(\"NaNs in X pre-split:\", int(X.isna().sum().sum()), \"| NaNs in y:\", int(y.isna().sum()))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "display((y_train.value_counts(normalize=True)*100).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e01a9",
   "metadata": {},
   "source": [
    "## 15) Preprocessing Plan (Impute → Encode → Scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "numeric_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "categorical_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "preprocessor = ColumnTransformer([(\"num\", numeric_pipe, num_cols), (\"cat\", categorical_pipe, cat_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f340ec7",
   "metadata": {},
   "source": [
    "## 16) Fit Preprocessor & Post‑Transform Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_train = preprocessor.fit_transform(X_train)\n",
    "Xt_test = preprocessor.transform(X_test)\n",
    "print(\"Transformed shapes:\", Xt_train.shape, Xt_test.shape)\n",
    "print(\"NaNs after preprocess (train)?\", bool(np.isnan(Xt_train).any()))\n",
    "print(\"NaNs after preprocess (test)?\", bool(np.isnan(Xt_test).any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59a040",
   "metadata": {},
   "source": [
    "## 17) Leakage Guardrails (Quick Scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_with_target = {}\n",
    "for c in X_train.select_dtypes(include=[np.number]).columns[:50]:\n",
    "    try:\n",
    "        corr_with_target[c] = abs(pd.concat([X_train[c], y_train], axis=1).corr().iloc[0,1])\n",
    "    except Exception:\n",
    "        pass\n",
    "suspicious = [k for k,v in corr_with_target.items() if v>=0.98]\n",
    "print(\"Near-perfect correlations with target:\", suspicious[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f91d34",
   "metadata": {},
   "source": [
    "## 18) Optional Procedural Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fe = X_train.copy(); X_test_fe = X_test.copy()\n",
    "age_cols = [c for c in X_train_fe.columns if \"age\" in c.lower()]\n",
    "for c in age_cols:\n",
    "    try:\n",
    "        X_train_fe[c+\"_bin\"] = pd.cut(X_train_fe[c],[0,30,40,50,60,70,200],labels=[\"<30\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70+\"], include_lowest=True)\n",
    "        X_test_fe[c+\"_bin\"] = pd.cut(X_test_fe[c],[0,30,40,50,60,70,200],labels=[\"<30\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70+\"], include_lowest=True)\n",
    "    except Exception as e:\n",
    "        print(\"Age binning skipped:\", e)\n",
    "bmi_cols = [c for c in X_train_fe.columns if \"bmi\" in c.lower()]\n",
    "for c in bmi_cols:\n",
    "    try:\n",
    "        X_train_fe[c+\"_class\"] = pd.cut(X_train_fe[c],[0,18.5,25,30,100],labels=[\"underweight\",\"normal\",\"overweight\",\"obese\"], include_lowest=True)\n",
    "        X_test_fe[c+\"_class\"] = pd.cut(X_test_fe[c],[0,18.5,25,30,100],labels=[\"underweight\",\"normal\",\"overweight\",\"obese\"], include_lowest=True)\n",
    "    except Exception as e:\n",
    "        print(\"BMI binning skipped:\", e)\n",
    "num_cols_fe = X_train_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "pairs = [(num_cols_fe[i], num_cols_fe[j]) for i in range(min(8,len(num_cols_fe))) for j in range(i+1, min(8,len(num_cols_fe)))]\n",
    "for a,b in pairs[:10]:\n",
    "    try:\n",
    "        X_train_fe[f\"ratio_{a}_over_{b}\"] = X_train_fe[a]/(X_train_fe[b].abs()+1e-6)\n",
    "        X_test_fe[f\"ratio_{a}_over_{b}\"] = X_test_fe[a]/(X_test_fe[b].abs()+1e-6)\n",
    "    except Exception:\n",
    "        pass\n",
    "print(\"NaNs after FE — train:\", int(X_train_fe.isna().sum().sum()), \"test:\", int(X_test_fe.isna().sum().sum()))\n",
    "num_cols2 = X_train_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols2 = X_train_fe.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "preprocessor_fe = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]), num_cols2),\n",
    "    (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols2)\n",
    "])\n",
    "Xt_train_fe = preprocessor_fe.fit_transform(X_train_fe)\n",
    "Xt_test_fe = preprocessor_fe.transform(X_test_fe)\n",
    "print(\"Shapes after FE+preprocess:\", Xt_train_fe.shape, Xt_test_fe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0e6ae",
   "metadata": {},
   "source": [
    "## 19) Model Suite (Pipelines with SMOTE inside CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719904b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = {\"accuracy\":\"accuracy\",\"precision\":\"precision\",\"recall\":\"recall\",\"f1\":\"f1\",\"roc_auc\":\"roc_auc\",\"average_precision\":\"average_precision\"}\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "pipe_logreg = ImbPipeline([(\"prep\", preprocessor_fe), (\"smote\", SMOTE(random_state=RANDOM_STATE)), (\"clf\", LogisticRegression(max_iter=1000, n_jobs=N_JOBS))])\n",
    "pipe_rf = ImbPipeline([(\"prep\", preprocessor_fe), (\"smote\", SMOTE(random_state=RANDOM_STATE)), (\"clf\", RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=N_JOBS))])\n",
    "models = {\"LogReg\": pipe_logreg, \"RF\": pipe_rf}\n",
    "if XGB_AVAILABLE:\n",
    "    pipe_xgb = ImbPipeline([(\"prep\", preprocessor_fe), (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "                            (\"clf\", XGBClassifier(n_estimators=500, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, max_depth=6, random_state=RANDOM_STATE, n_jobs=N_JOBS, eval_metric=\"logloss\"))])\n",
    "    models[\"XGB\"] = pipe_xgb\n",
    "list(models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2784720",
   "metadata": {},
   "source": [
    "## 20) Cross‑Validation — Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = {}; per_fold = {}\n",
    "for name, pipe in models.items():\n",
    "    scores = cross_validate(pipe, X_train_fe, y_train, cv=cv, scoring=scorer, n_jobs=N_JOBS)\n",
    "    cv_results[name] = {k: float(np.mean(v)) for k, v in scores.items()}\n",
    "    per_fold[name] = scores\n",
    "    print(name, \"CV means:\", cv_results[name])\n",
    "import pandas as pd\n",
    "cv_df = pd.DataFrame(cv_results).T.sort_values(\"test_roc_auc\", ascending=False)\n",
    "display(cv_df)\n",
    "best_name = cv_df.index[0]; best_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23450f",
   "metadata": {},
   "source": [
    "## 21) Cross‑Validation — Per Fold AUCs (Top Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "auc_vec = per_fold[best_name][\"test_roc_auc\"]\n",
    "fold_tbl = pd.DataFrame({\"fold\": np.arange(1, len(auc_vec)+1), \"roc_auc\": auc_vec})\n",
    "display(fold_tbl); print(\"Mean:\", float(np.mean(auc_vec)), \"Std:\", float(np.std(auc_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05e116",
   "metadata": {},
   "source": [
    "## 22) Threshold Analysis (validation folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d995a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Default threshold 0.5 used here. Adjust in deployment to meet recall/precision targets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e8fea",
   "metadata": {},
   "source": [
    "## 23) Fit Best Model on Train (with FE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = models[best_name]; best_pipe.fit(X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7ec79",
   "metadata": {},
   "source": [
    "## 24) Hold‑Out Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = best_pipe.predict_proba(X_test_fe)[:,1]; pred = (proba>=0.5).astype(int)\n",
    "print(\"Acc:\", accuracy_score(y_test, pred))\n",
    "print(\"Prec:\", precision_score(y_test, pred, zero_division=0))\n",
    "print(\"Rec:\", recall_score(y_test, pred, zero_division=0))\n",
    "print(\"F1:\", f1_score(y_test, pred, zero_division=0))\n",
    "print(\"ROC‑AUC:\", roc_auc_score(y_test, proba))\n",
    "print(\"PR‑AUC:\", average_precision_score(y_test, proba))\n",
    "plt.figure(); ConfusionMatrixDisplay.from_predictions(y_test, pred); plt.title(\"Confusion Matrix\"); plt.show()\n",
    "fpr, tpr, _ = roc_curve(y_test, proba); plt.figure(); plt.plot(fpr,tpr); plt.plot([0,1],[0,1],'--'); plt.title(\"ROC\"); plt.show()\n",
    "prec, rec, _ = precision_recall_curve(y_test, proba); plt.figure(); plt.plot(rec,prec); plt.title(\"PR Curve\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a8b51",
   "metadata": {},
   "source": [
    "## 25) Calibration Check (Reliability Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ebaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.0, 1.0, 11)\n",
    "digitized = np.digitize(proba, bins)-1\n",
    "rows = []\n",
    "for b in range(len(bins)-1):\n",
    "    mask = digitized==b\n",
    "    if mask.any():\n",
    "        rows.append((float(np.mean(proba[mask])), float(np.mean(y_test.iloc[mask]))))\n",
    "cal_tbl = pd.DataFrame(rows, columns=[\"avg_pred\",\"frac_positive\"])\n",
    "display(cal_tbl)\n",
    "plt.figure(); plt.plot(cal_tbl[\"avg_pred\"], cal_tbl[\"frac_positive\"], marker=\"o\"); plt.plot([0,1],[0,1],'--'); plt.title(\"Calibration\"); plt.show()\n",
    "print(\"Brier:\", brier_score_loss(y_test, proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c5986",
   "metadata": {},
   "source": [
    "## 26) Feature Importance / Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = best_pipe.named_steps[\"clf\"]\n",
    "if hasattr(clf, \"feature_importances_\"):\n",
    "    imp = clf.feature_importances_\n",
    "    topk = min(20, len(imp)); idx = np.argsort(imp)[-topk:][::-1]\n",
    "    plt.figure(figsize=(7,6)); plt.barh(range(topk), imp[idx][::-1])\n",
    "    plt.yticks(range(topk), [f\"f_{i}\" for i in idx][::-1]); plt.title(\"Top Importances (indices)\"); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"No native importances for this classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14c09b",
   "metadata": {},
   "source": [
    "## 27) Hyperparameter Tuning (RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca25abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TUNING = True\n",
    "if RUN_TUNING:\n",
    "    import scipy.stats as st\n",
    "    if best_name==\"RF\":\n",
    "        dist = {\"clf__n_estimators\": st.randint(300,900),\n",
    "                \"clf__max_depth\": st.randint(3,20),\n",
    "                \"clf__min_samples_split\": st.randint(2,20),\n",
    "                \"clf__min_samples_leaf\": st.randint(1,20),\n",
    "                \"clf__max_features\": [\"sqrt\",\"log2\",None]}\n",
    "    elif best_name==\"LogReg\":\n",
    "        dist = {\"clf__C\": st.loguniform(1e-3,1e2),\n",
    "                \"clf__solver\": [\"lbfgs\",\"liblinear\"],\n",
    "                \"clf__penalty\": [\"l2\"]}\n",
    "    elif best_name==\"XGB\" and XGB_AVAILABLE:\n",
    "        dist = {\"clf__n_estimators\": st.randint(300,900),\n",
    "                \"clf__max_depth\": st.randint(3,12),\n",
    "                \"clf__learning_rate\": st.uniform(0.01,0.2),\n",
    "                \"clf__subsample\": st.uniform(0.6,0.4),\n",
    "                \"clf__colsample_bytree\": st.uniform(0.6,0.4),\n",
    "                \"clf__gamma\": st.uniform(0.0,5.0),\n",
    "                \"clf__reg_alpha\": st.uniform(0.0,1.0),\n",
    "                \"clf__reg_lambda\": st.uniform(0.5,1.5)}\n",
    "    else:\n",
    "        dist = None\n",
    "    if dist:\n",
    "        tuner = RandomizedSearchCV(models[best_name], param_distributions=dist, n_iter=30,\n",
    "                                   scoring=\"roc_auc\",\n",
    "                                   cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "                                   random_state=RANDOM_STATE, n_jobs=N_JOBS, verbose=1)\n",
    "        tuner.fit(X_train_fe, y_train)\n",
    "        print(\"Best params:\", tuner.best_params_); print(\"Best CV ROC‑AUC:\", tuner.best_score_)\n",
    "        tuned = tuner.best_estimator_; proba_t = tuned.predict_proba(X_test_fe)[:,1]; pred_t = (proba_t>=0.5).astype(int)\n",
    "        print(\"\\n— Test (Tuned) —\")\n",
    "        print(\"Acc:\", accuracy_score(y_test, pred_t))\n",
    "        print(\"Prec:\", precision_score(y_test, pred_t, zero_division=0))\n",
    "        print(\"Rec:\", recall_score(y_test, pred_t, zero_division=0))\n",
    "        print(\"F1:\", f1_score(y_test, pred_t, zero_division=0))\n",
    "        print(\"ROC‑AUC:\", roc_auc_score(y_test, proba_t))\n",
    "        print(\"PR‑AUC:\", average_precision_score(y_test, proba_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d137d",
   "metadata": {},
   "source": [
    "## 29) Fairness & Subgroup Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_cols = [c for c in df.columns if any(k in c.lower() for k in [\"age\",\"sex\",\"ethnicity\",\"bmi\",\"region\"])][:3]\n",
    "print(\"Slice candidates:\", slice_cols)\n",
    "if slice_cols:\n",
    "    for sc in slice_cols:\n",
    "        if sc in X_test.columns:\n",
    "            tmp = pd.DataFrame({sc: X_test[sc], \"y_true\": y_test, \"y_prob\": proba})\n",
    "            for lvl, sub in tmp.groupby(sc):\n",
    "                if len(sub) < 30: \n",
    "                    continue\n",
    "                yhat = (sub[\"y_prob\"]>=0.5).astype(int)\n",
    "                print(f\"{sc}={lvl}: n={len(sub)}, Acc={accuracy_score(sub['y_true'], yhat):.3f}, Rec={recall_score(sub['y_true'], yhat, zero_division=0):.3f}\")\n",
    "else:\n",
    "    print(\"No obvious demographic columns detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19902803",
   "metadata": {},
   "source": [
    "## 30) Error Analysis: Inspect FP/FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = pd.DataFrame({\"y_true\": y_test.values, \"y_prob\": proba, \"y_pred\": (proba>=0.5).astype(int)}, index=X_test.index)\n",
    "fp = errs[(errs.y_true==0) & (errs.y_pred==1)].sort_values(\"y_prob\", ascending=False).head(10)\n",
    "fn = errs[(errs.y_true==1) & (errs.y_pred==0)].sort_values(\"y_prob\", ascending=True).head(10)\n",
    "print(\"False Positives (top 10 by prob):\"); display(fp)\n",
    "print(\"False Negatives (top 10 by prob):\"); display(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d902295",
   "metadata": {},
   "source": [
    "## 31) Model Persistence (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL = False\n",
    "if SAVE_MODEL:\n",
    "    import joblib, os\n",
    "    os.makedirs(\"artifacts\", exist_ok=True)\n",
    "    joblib.dump(best_pipe, \"artifacts/best_model_pipeline.joblib\")\n",
    "    print(\"Saved → artifacts/best_model_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8203e6f",
   "metadata": {},
   "source": [
    "## 32) Scaling to Full Dataset\n",
    "- Increase CV folds and tuning iterations if time permits.\n",
    "- Consider distributed compute or chunked reading.\n",
    "- Monitor memory; use sparse encodings where appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500c1ba",
   "metadata": {},
   "source": [
    "## 33) Risks, Limitations, Ethics\n",
    "- Validate across subgroups; avoid disparate impact.\n",
    "- Calibrate probabilities for clinical thresholds.\n",
    "- Document assumptions and data limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7ac99",
   "metadata": {},
   "source": [
    "## 34) Next Steps & Checklist\n",
    "- [ ] Set `DATA_PATH` to full dataset\n",
    "- [ ] Confirm `TARGET_COL` and its 0/1 mapping\n",
    "- [ ] Choose operating threshold to meet recall requirements\n",
    "- [ ] Persist final model and calibration\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37dae5a6",
   "metadata": {},
   "source": [
    "\n",
    "# Non‑Invasive Breast Cancer Prediction — Step‑by‑Step (No `def`)\n",
    "\n",
    "**Date:** 2025-08-08 19:20\n",
    "\n",
    "Procedural, section‑by‑section analysis using the 10% sample.  \n",
    "No custom functions or classes are defined; only standard library and package APIs are used.\n",
    "\n",
    "**Sections**\n",
    "1. Setup & Config  \n",
    "2. Load Data  \n",
    "3. First Look (head/tail/info/dtypes/describe)  \n",
    "4. Missing Values (NaN) Audit  \n",
    "5. Basic Distributions & Correlations  \n",
    "6. Target Column Detection & Class Balance  \n",
    "7. Train/Test Split  \n",
    "8. Preprocessing (Impute/Encode/Scale)  \n",
    "9. Optional Feature Engineering (procedural)  \n",
    "10. SMOTE (Class Imbalance)  \n",
    "11. Baseline Models (LogReg, RandomForest, XGBoost if available) with Cross‑Validation  \n",
    "12. Hold‑out Evaluation (Confusion Matrix, ROC, PR)  \n",
    "13. Hyperparameter Tuning (RandomizedSearchCV)  \n",
    "14. Final Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f89b2",
   "metadata": {},
   "source": [
    "## 1) Setup & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths and parameters\n",
    "DATA_PATH = \"/mnt/data/sample_10percent.csv\"  # change to full dataset later\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 5\n",
    "TEST_SIZE = 0.2\n",
    "MAX_ROWS_FOR_PLOTS = 50000  # to keep plots responsive\n",
    "N_JOBS = -1\n",
    "\n",
    "# Libraries\n",
    "import warnings, os, math, itertools\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, average_precision_score, ConfusionMatrixDisplay)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional: XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not available; install with: pip install xgboost\")\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# Imbalanced-learn for SMOTE\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    IMB_OK = True\n",
    "except Exception as e:\n",
    "    print(\"imbalanced-learn not available; install with: pip install imbalanced-learn\")\n",
    "    IMB_OK = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321fe4d",
   "metadata": {},
   "source": [
    "## 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a0398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "display(df.head(3))\n",
    "display(df.tail(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d1ae4",
   "metadata": {},
   "source": [
    "## 3) First Look (head/tail/info/dtypes/describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = df.info()\n",
    "display(pd.DataFrame(df.dtypes, columns=[\"dtype\"]).T)\n",
    "display(df.describe(include='number').T.head(20))\n",
    "display(df.describe(include='object').T.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497171f",
   "metadata": {},
   "source": [
    "## 4) Missing Values (NaN) Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing_counts / len(df)) * 100\n",
    "missing_table = pd.DataFrame({\"missing_count\": missing_counts, \"missing_pct\": missing_pct})\n",
    "display(missing_table.head(30))\n",
    "\n",
    "# Quick checks: any completely empty columns or rows?\n",
    "empty_cols = missing_counts[missing_counts == len(df)].index.tolist()\n",
    "print(\"Completely empty columns:\", empty_cols)\n",
    "\n",
    "any_row_all_nan = df.isna().all(axis=1).any()\n",
    "print(\"Any row entirely NaN?:\", any_row_all_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b68bc",
   "metadata": {},
   "source": [
    "## 5) Basic Distributions & Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"Numeric count:\", len(numeric_cols), \"Categorical count:\", len(categorical_cols))\n",
    "\n",
    "# Sample for plotting\n",
    "plot_df = df\n",
    "if len(df) > MAX_ROWS_FOR_PLOTS:\n",
    "    plot_df = df.sample(n=MAX_ROWS_FOR_PLOTS, random_state=RANDOM_STATE)\n",
    "\n",
    "# Histograms for up to 8 numeric columns\n",
    "for col in numeric_cols[:8]:\n",
    "    plt.figure()\n",
    "    plot_df[col].hist(bins=40)\n",
    "    plt.title(f\"Distribution: {col}\")\n",
    "    plt.xlabel(col); plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "# Bar charts for up to 6 categoricals\n",
    "for col in categorical_cols[:6]:\n",
    "    plt.figure()\n",
    "    plot_df[col].astype(str).value_counts().head(15).plot(kind='bar')\n",
    "    plt.title(f\"Top categories: {col}\")\n",
    "    plt.xlabel(col); plt.ylabel(\"count\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation heatmap for numerics (limited to first 20 to keep readable)\n",
    "subset_cols = numeric_cols[:20]\n",
    "if len(subset_cols) >= 2:\n",
    "    corr = plot_df[subset_cols].corr()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    im = plt.imshow(corr, aspect='auto')\n",
    "    plt.colorbar(im)\n",
    "    plt.title(\"Correlation heatmap (subset of numeric features)\")\n",
    "    plt.xticks(range(len(subset_cols)), [c[:12] for c in subset_cols], rotation=90)\n",
    "    plt.yticks(range(len(subset_cols)), [c[:12] for c in subset_cols])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough numeric columns for heatmap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561d950",
   "metadata": {},
   "source": [
    "## 6) Target Column Detection & Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET_COL = None\n",
    "\n",
    "candidate_names = [\"target\",\"label\",\"cancer\",\"malignant\",\"y\",\"outcome\"]\n",
    "for c in df.columns:\n",
    "    if c.lower() in candidate_names:\n",
    "        TARGET_COL = c\n",
    "        break\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    last_col = df.columns[-1]\n",
    "    uniq = pd.Series(df[last_col].dropna().unique())\n",
    "    looks_binary = len(uniq) <= 5 and set(uniq.astype(str)) <= set(map(str,[0,1,\"0\",\"1\",\"yes\",\"no\",\"True\",\"False\"]))\n",
    "    TARGET_COL = last_col if looks_binary else None\n",
    "\n",
    "print(\"Detected TARGET_COL:\", TARGET_COL)\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    raise ValueError(\"Please set a binary TARGET_COL manually in this cell and re-run.\")\n",
    "\n",
    "print(\"Target value counts:\")\n",
    "display(df[TARGET_COL].value_counts(dropna=False))\n",
    "\n",
    "print(\"Target distribution (%):\")\n",
    "display((df[TARGET_COL].value_counts(normalize=True) * 100).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ca079",
   "metadata": {},
   "source": [
    "## 7) Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ID_COLUMNS = []  # add any id-like columns to drop\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL] + [c for c in ID_COLUMNS if c in df.columns], errors='ignore')\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Basic NaN check before split\n",
    "print(\"NaNs in X before split:\", int(X.isna().sum().sum()))\n",
    "print(\"NaNs in y before split:\", int(y.isna().sum()))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shapes:\", X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"Class balance (train):\")\n",
    "display(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Class balance (test):\")\n",
    "display(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc059f",
   "metadata": {},
   "source": [
    "## 8) Preprocessing (Impute/Encode/Scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"Numerical:\", len(num_cols), \"Categorical:\", len(cat_cols))\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the preprocessor to training data and transform both sets to inspect NaNs\n",
    "Xt_train = preprocessor.fit_transform(X_train)\n",
    "Xt_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Transformed shapes:\", Xt_train.shape, Xt_test.shape)\n",
    "\n",
    "# Convert to DataFrame with generated column names when possible\n",
    "feature_names = []\n",
    "for name, trans, cols in preprocessor.transformers_:\n",
    "    if hasattr(trans, \"named_steps\"):\n",
    "        # try to fetch names from last step if available\n",
    "        steps = list(trans.named_steps.values())\n",
    "        last = steps[-1]\n",
    "        if hasattr(last, \"get_feature_names_out\"):\n",
    "            try:\n",
    "                arr = last.get_feature_names_out(cols)\n",
    "                feature_names.extend(list(arr))\n",
    "            except Exception:\n",
    "                feature_names.extend(list(cols))\n",
    "        else:\n",
    "            feature_names.extend(list(cols))\n",
    "    else:\n",
    "        feature_names.extend(list(cols))\n",
    "\n",
    "Xt_train_df = pd.DataFrame(Xt_train, columns=[str(c) for c in feature_names])\n",
    "Xt_test_df = pd.DataFrame(Xt_test, columns=[str(c) for c in feature_names])\n",
    "\n",
    "print(\"Any NaNs after preprocessing — train?:\", Xt_train_df.isna().any().any())\n",
    "print(\"Any NaNs after preprocessing — test?:\", Xt_test_df.isna().any().any())\n",
    "\n",
    "display(Xt_train_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ded33",
   "metadata": {},
   "source": [
    "## 9) Optional Feature Engineering (procedural; no `def`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d08a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a working copy for FE\n",
    "X_train_fe = X_train.copy()\n",
    "X_test_fe = X_test.copy()\n",
    "\n",
    "# Age bins if a column containing 'age' exists\n",
    "age_cols = [c for c in X_train_fe.columns if \"age\" in c.lower()]\n",
    "for c in age_cols:\n",
    "    try:\n",
    "        bins = [0, 30, 40, 50, 60, 70, 200]\n",
    "        labels = [\"<30\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70+\"]\n",
    "        X_train_fe[c + \"_bin\"] = pd.cut(X_train_fe[c], bins=bins, labels=labels, include_lowest=True)\n",
    "        X_test_fe[c + \"_bin\"] = pd.cut(X_test_fe[c], bins=bins, labels=labels, include_lowest=True)\n",
    "    except Exception as e:\n",
    "        print(\"Age binning failed for\", c, \":\", e)\n",
    "\n",
    "# BMI classes if a column containing 'bmi' exists\n",
    "bmi_cols = [c for c in X_train_fe.columns if \"bmi\" in c.lower()]\n",
    "for c in bmi_cols:\n",
    "    try:\n",
    "        bins = [0, 18.5, 25, 30, 100]\n",
    "        labels = [\"underweight\",\"normal\",\"overweight\",\"obese\"]\n",
    "        X_train_fe[c + \"_class\"] = pd.cut(X_train_fe[c], bins=bins, labels=labels, include_lowest=True)\n",
    "        X_test_fe[c + \"_class\"] = pd.cut(X_test_fe[c], bins=bins, labels=labels, include_lowest=True)\n",
    "    except Exception as e:\n",
    "        print(\"BMI binning failed for\", c, \":\", e)\n",
    "\n",
    "# A few safe ratios across first few numeric pairs to avoid explosion\n",
    "num_cols_fe = X_train_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "pairs = list(itertools.combinations(num_cols_fe[:10], 2))  # limit\n",
    "for a, b in pairs[:10]:\n",
    "    try:\n",
    "        new_col = f\"ratio_{a}_over_{b}\"\n",
    "        X_train_fe[new_col] = X_train_fe[a] / (X_train_fe[b].abs() + 1e-6)\n",
    "        X_test_fe[new_col]  = X_test_fe[a]  / (X_test_fe[b].abs() + 1e-6)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "print(\"NaNs after FE (train):\", int(X_train_fe.isna().sum().sum()))\n",
    "print(\"NaNs after FE (test):\", int(X_test_fe.isna().sum().sum()))\n",
    "\n",
    "# Rebuild preprocessors on FE data\n",
    "num_cols2 = X_train_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols2 = X_train_fe.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "preprocessor_fe = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]), num_cols2),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Xt_train_fe = preprocessor_fe.fit_transform(X_train_fe)\n",
    "Xt_test_fe = preprocessor_fe.transform(X_test_fe)\n",
    "\n",
    "print(\"Shapes after FE+preprocess:\", Xt_train_fe.shape, Xt_test_fe.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3577f",
   "metadata": {},
   "source": [
    "## 10) SMOTE (Class Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb885ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if IMB_OK:\n",
    "    # We'll integrate SMOTE inside model pipelines during CV to avoid leakage.\n",
    "    # Here we only show class balance pre‑SMOTE as a check.\n",
    "    print(\"Train class distribution before SMOTE:\")\n",
    "    display(y_train.value_counts(normalize=True))\n",
    "else:\n",
    "    print(\"SMOTE unavailable. Install imbalanced-learn to enable it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc28964",
   "metadata": {},
   "source": [
    "## 11) Baseline Models (Cross‑Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scorer = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "    \"average_precision\": \"average_precision\"\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Logistic Regression pipeline (with SMOTE inside)\n",
    "pipe_logreg = ImbPipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_fe),\n",
    "    (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, n_jobs=N_JOBS))\n",
    "])\n",
    "\n",
    "scores_lr = cross_validate(pipe_logreg, X_train_fe, y_train, cv=cv, scoring=scorer, n_jobs=N_JOBS)\n",
    "print(\"LogReg CV (mean):\", {k: float(np.mean(v)) for k,v in scores_lr.items()})\n",
    "\n",
    "# Random Forest pipeline\n",
    "pipe_rf = ImbPipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_fe),\n",
    "    (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=N_JOBS))\n",
    "])\n",
    "scores_rf = cross_validate(pipe_rf, X_train_fe, y_train, cv=cv, scoring=scorer, n_jobs=N_JOBS)\n",
    "print(\"RF CV (mean):\", {k: float(np.mean(v)) for k,v in scores_rf.items()})\n",
    "\n",
    "# XGBoost pipeline (if available)\n",
    "if XGB_AVAILABLE:\n",
    "    pipe_xgb = ImbPipeline(steps=[\n",
    "        (\"preprocess\", preprocessor_fe),\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\", XGBClassifier(\n",
    "            n_estimators=400, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
    "            max_depth=6, random_state=RANDOM_STATE, n_jobs=N_JOBS, eval_metric=\"logloss\"\n",
    "        ))\n",
    "    ])\n",
    "    scores_xgb = cross_validate(pipe_xgb, X_train_fe, y_train, cv=cv, scoring=scorer, n_jobs=N_JOBS)\n",
    "    print(\"XGB CV (mean):\", {k: float(np.mean(v)) for k,v in scores_xgb.items()})\n",
    "else:\n",
    "    scores_xgb = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b280bd7",
   "metadata": {},
   "source": [
    "## 12) Hold‑out Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5282d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {}\n",
    "\n",
    "# Choose best model by CV ROC-AUC\n",
    "roc_means = []\n",
    "roc_means.append((\"LogReg\", float(np.mean(scores_lr['test_roc_auc']))))\n",
    "roc_means.append((\"RF\", float(np.mean(scores_rf['test_roc_auc']))))\n",
    "if scores_xgb is not None:\n",
    "    roc_means.append((\"XGB\", float(np.mean(scores_xgb['test_roc_auc']))))\n",
    "\n",
    "roc_means = sorted(roc_means, key=lambda x: x[1], reverse=True)\n",
    "print(\"CV ROC-AUC ranking:\", roc_means)\n",
    "\n",
    "best_name = roc_means[0][0]\n",
    "print(\"Selected model:\", best_name)\n",
    "\n",
    "if best_name == \"LogReg\":\n",
    "    final_pipe = pipe_logreg\n",
    "elif best_name == \"RF\":\n",
    "    final_pipe = pipe_rf\n",
    "else:\n",
    "    final_pipe = pipe_xgb\n",
    "\n",
    "final_pipe.fit(X_train_fe, y_train)\n",
    "\n",
    "y_proba = final_pipe.predict_proba(X_test_fe)[:,1]\n",
    "y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test Precision:\", precision_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Test Recall:\", recall_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Test F1:\", f1_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"Test PR-AUC:\", average_precision_score(y_test, y_proba))\n",
    "\n",
    "plt.figure()\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title(f\"Confusion Matrix — {best_name}\")\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"ROC\")\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(f\"ROC Curve — {best_name}\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=\"PR\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"Precision-Recall Curve — {best_name}\")\n",
    "plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9d340",
   "metadata": {},
   "source": [
    "## 13) Hyperparameter Tuning (RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "model_to_tune = best_name\n",
    "\n",
    "if model_to_tune == \"RF\":\n",
    "    param_dist = {\n",
    "        \"clf__n_estimators\": st.randint(200, 800),\n",
    "        \"clf__max_depth\": st.randint(3, 20),\n",
    "        \"clf__min_samples_split\": st.randint(2, 20),\n",
    "        \"clf__min_samples_leaf\": st.randint(1, 20),\n",
    "        \"clf__max_features\": [\"sqrt\", \"log2\", None],\n",
    "    }\n",
    "    base = pipe_rf\n",
    "elif model_to_tune == \"LogReg\":\n",
    "    param_dist = {\n",
    "        \"clf__C\": st.loguniform(1e-3, 1e2),\n",
    "        \"clf__penalty\": [\"l2\"],\n",
    "        \"clf__solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    }\n",
    "    base = pipe_logreg\n",
    "elif model_to_tune == \"XGB\" and XGB_AVAILABLE:\n",
    "    param_dist = {\n",
    "        \"clf__n_estimators\": st.randint(200, 800),\n",
    "        \"clf__max_depth\": st.randint(3, 12),\n",
    "        \"clf__learning_rate\": st.uniform(0.01, 0.2),\n",
    "        \"clf__subsample\": st.uniform(0.6, 0.4),\n",
    "        \"clf__colsample_bytree\": st.uniform(0.6, 0.4),\n",
    "        \"clf__gamma\": st.uniform(0.0, 5.0),\n",
    "        \"clf__reg_alpha\": st.uniform(0.0, 1.0),\n",
    "        \"clf__reg_lambda\": st.uniform(0.5, 1.5),\n",
    "    }\n",
    "    base = pipe_xgb\n",
    "else:\n",
    "    param_dist = None\n",
    "\n",
    "if param_dist is None:\n",
    "    print(\"No hyperparameter space for the selected model; skipping.\")\n",
    "else:\n",
    "    tuner = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=25,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        verbose=1\n",
    "    )\n",
    "    tuner.fit(X_train_fe, y_train)\n",
    "    print(\"Best params:\", tuner.best_params_)\n",
    "    print(\"Best CV ROC-AUC:\", tuner.best_score_)\n",
    "\n",
    "    tuned = tuner.best_estimator_\n",
    "    y_proba_tuned = tuned.predict_proba(X_test_fe)[:,1]\n",
    "    y_pred_tuned = (y_proba_tuned >= 0.5).astype(int)\n",
    "\n",
    "    print(\"\\n=== Test (Tuned) ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred_tuned, zero_division=0))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred_tuned, zero_division=0))\n",
    "    print(\"F1:\", f1_score(y_test, y_pred_tuned, zero_division=0))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_tuned))\n",
    "    print(\"PR-AUC:\", average_precision_score(y_test, y_proba_tuned))\n",
    "\n",
    "    plt.figure()\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred_tuned)\n",
    "    plt.title(f\"Confusion Matrix — {model_to_tune} (Tuned)\")\n",
    "    plt.show()\n",
    "\n",
    "    fpr2, tpr2, _ = roc_curve(y_test, y_proba_tuned)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr2, tpr2, label=\"ROC (tuned)\")\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(f\"ROC Curve — {model_to_tune} (Tuned)\")\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "    prec2, rec2, _ = precision_recall_curve(y_test, y_proba_tuned)\n",
    "    plt.figure()\n",
    "    plt.plot(rec2, prec2, label=\"PR (tuned)\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"Precision-Recall Curve — {model_to_tune} (Tuned)\")\n",
    "    plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380bdda9",
   "metadata": {},
   "source": [
    "## 14) Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b100681",
   "metadata": {},
   "source": [
    "\n",
    "- No custom `def`/UDFs or classes were used; everything is procedural with scikit‑learn/imbalanced‑learn components.\n",
    "- NaN checks are included: before split, after preprocessing, after FE, and around modeling outputs.\n",
    "- When scaling to ~1.5M rows, consider batching, increasing CV folds/iterations if runtime permits, and enabling model logging.\n",
    "- If you know the exact target column, set it explicitly in **Section 6** for reliability.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0823c34d",
   "metadata": {},
   "source": [
    "\n",
    "# Early Breast Cancer Risk Stratification (Non-Invasive) — MSc AI Project\n",
    "\n",
    "**Author:** _Your Name_  \n",
    "**Date created:** 2025-08-08 19:15\n",
    "\n",
    "This notebook builds an end‑to‑end pipeline for non-invasive breast cancer prediction using a 10% development subset of a large dataset (≈1.5M rows total).  \n",
    "You will run the same notebook later on the full dataset.\n",
    "\n",
    "**Contents**\n",
    "1. Setup & Config\n",
    "2. Data Loading\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Preprocessing & Train/Test Split\n",
    "5. Feature Engineering\n",
    "6. Feature Selection & Dimensionality Reduction\n",
    "7. Modeling (Baseline → Cross‑Validation → Evaluation)\n",
    "8. Hyperparameter Optimization\n",
    "9. Final Comparison & Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf3096",
   "metadata": {},
   "source": [
    "## 1) Setup & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== USER CONFIG ====\n",
    "DATA_PATH = \"/mnt/data/sample_10percent.csv\"  # change to full dataset later\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COL = None  # e.g., \"cancer\" or \"label\" (1=positive, 0=negative). If None, we'll try to infer.\n",
    "ID_COLUMNS = []    # list of id-like columns to drop from features\n",
    "MAX_ROWS_TO_PROFILE = 200000  # safety for EDA sampling\n",
    "CV_FOLDS = 5\n",
    "N_JOBS = -1  # use all cores where possible\n",
    "\n",
    "# ==== LIBRARIES ====\n",
    "import warnings, os, math, itertools\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, average_precision_score, ConfusionMatrixDisplay)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model families\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional: XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not available; will skip those steps. Install with: pip install xgboost\")\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# Imbalanced-learn\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"imbalanced-learn is required. Install with: pip install imbalanced-learn\")\n",
    "\n",
    "\n",
    "def safe_sample(df: pd.DataFrame, n: int):\n",
    "    if len(df) <= n:\n",
    "        return df\n",
    "    return df.sample(n=n, random_state=RANDOM_STATE)\n",
    "\n",
    "def infer_target_column(df: pd.DataFrame):\n",
    "    \"\"\"Heuristics to infer a binary target column.\"\"\"\n",
    "    candidates = [c for c in df.columns if c.lower() in {\"target\",\"label\",\"cancer\",\"malignant\",\"y\",\"outcome\"}]\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    # Fall back to last column if it looks binary\n",
    "    last = df.columns[-1]\n",
    "    unique_vals = df[last].dropna().unique()\n",
    "    if len(unique_vals) <= 5 and set(pd.Series(unique_vals).dropna().astype(str)) <= set(map(str,[0,1,\"0\",\"1\",\"yes\",\"no\",\"True\",\"False\"])):\n",
    "        return last\n",
    "    return None\n",
    "\n",
    "def classify_columns(df: pd.DataFrame, id_cols=None, target_col=None):\n",
    "    id_cols = id_cols or []\n",
    "    feature_df = df.drop(columns=[c for c in id_cols if c in df.columns], errors='ignore')\n",
    "    if target_col and target_col in feature_df.columns:\n",
    "        feature_df = feature_df.drop(columns=[target_col])\n",
    "    numeric_cols = feature_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = feature_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "print(\"Config loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989cffa",
   "metadata": {},
   "source": [
    "## 2) Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277122bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TARGET_COL is None:\n",
    "    TARGET_COL = infer_target_column(df)\n",
    "print(\"TARGET_COL inferred as:\", TARGET_COL)\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    raise ValueError(\"Please set TARGET_COL to the binary label column (e.g., 'cancer', 'target').\")\n",
    "\n",
    "print(\"Class balance:\")\n",
    "print(df[TARGET_COL].value_counts(dropna=False, normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c857c",
   "metadata": {},
   "source": [
    "## 3) Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d49bf",
   "metadata": {},
   "source": [
    "\n",
    "Explore structure, missingness, data types, and distributions. Plots operate on a sampled view if the dataset is large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.dtypes.to_frame(\"dtype\").T)\n",
    "display(df.describe(include='all').transpose().head(30))\n",
    "\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing_counts / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\"missing_count\": missing_counts, \"missing_pct\": missing_pct})\n",
    "display(missing_df.head(20))\n",
    "\n",
    "num_cols, cat_cols = classify_columns(df, id_cols=ID_COLUMNS, target_col=TARGET_COL)\n",
    "print(\"Numeric columns (first 20):\", num_cols[:20])\n",
    "print(\"Categorical columns (first 20):\", cat_cols[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_df = safe_sample(df[num_cols + [TARGET_COL]], n=min(MAX_ROWS_TO_PROFILE, 50000))\n",
    "\n",
    "n_show = min(8, len(num_cols))\n",
    "for col in num_cols[:n_show]:\n",
    "    plt.figure()\n",
    "    sample_df[col].hist(bins=40)\n",
    "    plt.title(f\"Distribution: {col}\")\n",
    "    plt.xlabel(col); plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "n_show_cat = min(6, len(cat_cols))\n",
    "for col in cat_cols[:n_show_cat]:\n",
    "    plt.figure()\n",
    "    sample_df[col].astype(str).value_counts().head(15).plot(kind='bar')\n",
    "    plt.title(f\"Top categories: {col}\")\n",
    "    plt.xlabel(col); plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(num_cols) >= 2:\n",
    "    corr = sample_df[num_cols].corr()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    im = plt.imshow(corr, aspect='auto')\n",
    "    plt.colorbar(im)\n",
    "    plt.title(\"Correlation heatmap (numeric)\")\n",
    "    plt.xticks(range(len(num_cols)), [c[:10] for c in num_cols], rotation=90)\n",
    "    plt.yticks(range(len(num_cols)), [c[:10] for c in num_cols])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough numeric columns for correlation heatmap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef556c7",
   "metadata": {},
   "source": [
    "## 4) Preprocessing & Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ef265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=[TARGET_COL] + [c for c in ID_COLUMNS if c in df.columns], errors='ignore')\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "num_cols, cat_cols = classify_columns(df, id_cols=ID_COLUMNS, target_col=TARGET_COL)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipelines built.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398e067",
   "metadata": {},
   "source": [
    "## 5) Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b65e4a",
   "metadata": {},
   "source": [
    "\n",
    "Adds safe, generic features (ratios/interactions) and age/BMI bins if those columns exist.\n",
    "Wrapped in a transformer for clean pipeline integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddd454",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class SimpleFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols, max_interactions=10, eps=1e-6):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.max_interactions = max_interactions\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.pairs_ = list(itertools.combinations(self.numeric_cols, 2))[:self.max_interactions]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for a, b in self.pairs_:\n",
    "            if a in X_.columns and b in X_.columns:\n",
    "                new_col = f\"ratio_{a}_over_{b}\"\n",
    "                X_[new_col] = X_[a] / (X_[b].abs() + self.eps)\n",
    "\n",
    "        age_candidates = [c for c in X_.columns if \"age\" in c.lower()]\n",
    "        for c in age_candidates:\n",
    "            try:\n",
    "                bins = [0, 30, 40, 50, 60, 70, 200]\n",
    "                labels = [\"<30\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70+\"]\n",
    "                X_[f\"{c}_bin\"] = pd.cut(X_[c], bins=bins, labels=labels, include_lowest=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        bmi_candidates = [c for c in X_.columns if \"bmi\" in c.lower()]\n",
    "        for c in bmi_candidates:\n",
    "            try:\n",
    "                bins = [0, 18.5, 25, 30, 100]\n",
    "                labels = [\"underweight\",\"normal\",\"overweight\",\"obese\"]\n",
    "                X_[f\"{c}_class\"] = pd.cut(X_[c], bins=bins, labels=labels, include_lowest=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return X_\n",
    "\n",
    "fe = SimpleFeatureEngineer(numeric_cols=num_cols, max_interactions=10)\n",
    "X_train_fe = fe.fit_transform(X_train)\n",
    "X_test_fe = fe.transform(X_test)\n",
    "\n",
    "num_cols_fe, cat_cols_fe = classify_columns(pd.concat([X_train_fe, y_train], axis=1), target_col=TARGET_COL)\n",
    "preprocessor_fe = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]), [c for c in num_cols_fe if c in X_train_fe.columns]),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), [c for c in cat_cols_fe if c in X_train_fe.columns])\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Feature engineering complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f7c8dc",
   "metadata": {},
   "source": [
    "## 6) Feature Selection & Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52552b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "USE_SELECTKBEST = True\n",
    "USE_PCA = False\n",
    "select_k = 50\n",
    "fs_selector = SelectKBest(score_func=mutual_info_classif, k=min(select_k,  max(5, len(num_cols_fe))))\n",
    "n_pca_components = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d53dd6",
   "metadata": {},
   "source": [
    "## 7) Modeling (Baseline → Cross‑Validation → Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, average_precision_score, ConfusionMatrixDisplay)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def build_pipeline(estimator, use_fe=True, use_selectk=USE_SELECTKBEST, use_pca=USE_PCA):\n",
    "    steps = []\n",
    "    if use_fe:\n",
    "        steps.append((\"fe\", SimpleFeatureEngineer(numeric_cols=num_cols, max_interactions=10)))\n",
    "        steps.append((\"preprocess\", preprocessor_fe))\n",
    "    else:\n",
    "        steps.append((\"preprocess\", preprocessor))\n",
    "\n",
    "    if use_selectk:\n",
    "        steps.append((\"selectk\", SelectKBest(score_func=mutual_info_classif, k=min(50,  max(5, len(num_cols_fe))))))\n",
    "\n",
    "    if use_pca:\n",
    "        steps.append((\"pca\", PCA(n_components=n_pca_components, random_state=RANDOM_STATE)))\n",
    "\n",
    "    steps.extend([\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\", estimator)\n",
    "    ])\n",
    "    return ImbPipeline(steps)\n",
    "\n",
    "models = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=1000, n_jobs=N_JOBS),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=N_JOBS)\n",
    "}\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    models[\"XGB\"] = XGBClassifier(\n",
    "        n_estimators=400, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
    "        max_depth=6, random_state=RANDOM_STATE, n_jobs=N_JOBS, eval_metric=\"logloss\"\n",
    "    )\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "scorer = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "    \"average_precision\": \"average_precision\"\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cv_results = {}\n",
    "for name, est in models.items():\n",
    "    pipe = build_pipeline(estimator=est, use_fe=True, use_selectk=USE_SELECTKBEST, use_pca=USE_PCA)\n",
    "    scores = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scorer, n_jobs=N_JOBS, return_train_score=False)\n",
    "    cv_results[name] = {k: float(np.mean(v)) for k, v in scores.items()}\n",
    "    print(name, \"CV metrics (mean over folds):\", cv_results[name])\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).T.sort_values(\"test_roc_auc\", ascending=False)\n",
    "display(cv_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_name = cv_df.index[0]\n",
    "print(\"Best model by CV ROC-AUC:\", best_name)\n",
    "best_estimator = models[best_name]\n",
    "\n",
    "final_pipe = build_pipeline(estimator=best_estimator, use_fe=True, use_selectk=USE_SELECTKBEST, use_pca=USE_PCA)\n",
    "final_pipe.fit(X_train, y_train)\n",
    "\n",
    "pred_proba = final_pipe.predict_proba(X_test)[:,1]\n",
    "y_pred = (pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test Precision:\", precision_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Test Recall:\", recall_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Test F1:\", f1_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Test ROC-AUC:\", roc_auc_score(y_test, pred_proba))\n",
    "print(\"Test PR-AUC:\", average_precision_score(y_test, pred_proba))\n",
    "\n",
    "plt.figure()\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title(f\"Confusion Matrix — {best_name}\")\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"ROC\")\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(f\"ROC Curve — {best_name}\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=\"PR\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"Precision-Recall Curve — {best_name}\")\n",
    "plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf413387",
   "metadata": {},
   "source": [
    "### Feature Importance (if supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "def get_feature_names_after_preprocessing(preprocessor, X_cols):\n",
    "    output_features = []\n",
    "    for name, trans, cols in preprocessor.transformers_:\n",
    "        if name == \"remainder\":\n",
    "            continue\n",
    "        if hasattr(trans, \"named_steps\"):\n",
    "            last = list(trans.named_steps.values())[-1]\n",
    "            if hasattr(last, \"get_feature_names_out\"):\n",
    "                feat_names = last.get_feature_names_out(cols)\n",
    "            else:\n",
    "                feat_names = cols\n",
    "        else:\n",
    "            feat_names = cols\n",
    "        output_features.extend(feat_names)\n",
    "    return list(map(str, output_features))\n",
    "\n",
    "pipe_for_features = build_pipeline(estimator=best_estimator, use_fe=True, use_selectk=False, use_pca=False)\n",
    "pipe_for_features.fit(X_train, y_train)\n",
    "\n",
    "X_train_fe2 = pipe_for_features.named_steps[\"fe\"].transform(X_train)\n",
    "pre_fe_cols = X_train_fe2.columns.tolist()\n",
    "ct = pipe_for_features.named_steps[\"preprocess\"]\n",
    "feature_names = get_feature_names_after_preprocessing(ct, pre_fe_cols)\n",
    "\n",
    "if USE_SELECTKBEST:\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=min(50, max(5, len(num_cols_fe))))\n",
    "    Xt = selector.fit_transform(ct.fit_transform(X_train_fe2), y_train)\n",
    "    support = selector.get_support()\n",
    "    feature_names = [f for f, s in zip(feature_names, support) if s]\n",
    "\n",
    "fimp = None\n",
    "clf = final_pipe.named_steps[\"clf\"]\n",
    "if hasattr(clf, \"feature_importances_\"):\n",
    "    fimp = clf.feature_importances_\n",
    "elif hasattr(clf, \"coef_\"):\n",
    "    fimp = np.abs(clf.coef_).ravel()\n",
    "\n",
    "if fimp is not None:\n",
    "    topk = min(20, len(fimp))\n",
    "    idx = np.argsort(fimp)[-topk:][::-1]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(range(topk), np.array(fimp)[idx][::-1])\n",
    "    plt.yticks(range(topk), [str(feature_names[i])[:40] for i in idx][::-1])\n",
    "    plt.title(f\"Top {topk} Feature Importances — {best_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importances not available for this estimator.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe08a1a",
   "metadata": {},
   "source": [
    "## 8) Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "search_spaces = {\n",
    "    \"RF\": {\n",
    "        \"clf__n_estimators\": st.randint(200, 800),\n",
    "        \"clf__max_depth\": st.randint(3, 20),\n",
    "        \"clf__min_samples_split\": st.randint(2, 20),\n",
    "        \"clf__min_samples_leaf\": st.randint(1, 20),\n",
    "        \"clf__max_features\": [\"sqrt\", \"log2\", None],\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    search_spaces[\"XGB\"] = {\n",
    "        \"clf__n_estimators\": st.randint(200, 800),\n",
    "        \"clf__max_depth\": st.randint(3, 12),\n",
    "        \"clf__learning_rate\": st.uniform(0.01, 0.2),\n",
    "        \"clf__subsample\": st.uniform(0.6, 0.4),\n",
    "        \"clf__colsample_bytree\": st.uniform(0.6, 0.4),\n",
    "        \"clf__gamma\": st.uniform(0.0, 5.0),\n",
    "        \"clf__reg_alpha\": st.uniform(0.0, 1.0),\n",
    "        \"clf__reg_lambda\": st.uniform(0.5, 1.5),\n",
    "    }\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model_to_tune = best_name\n",
    "if model_to_tune not in search_spaces:\n",
    "    print(f\"No search space defined for {model_to_tune}. Skipping HPO.\")\n",
    "else:\n",
    "    base_estimator = models[model_to_tune]\n",
    "    tune_pipe = build_pipeline(estimator=base_estimator, use_fe=True, use_selectk=USE_SELECTKBEST, use_pca=USE_PCA)\n",
    "\n",
    "    rand_search = RandomizedSearchCV(\n",
    "        estimator=tune_pipe,\n",
    "        param_distributions=search_spaces[model_to_tune],\n",
    "        n_iter=25,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        verbose=1\n",
    "    )\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best params:\", rand_search.best_params_)\n",
    "    print(\"Best CV ROC-AUC:\", rand_search.best_score_)\n",
    "\n",
    "    tuned_model = rand_search.best_estimator_\n",
    "    pred_proba_tuned = tuned_model.predict_proba(X_test)[:,1]\n",
    "    y_pred_tuned = (pred_proba_tuned >= 0.5).astype(int)\n",
    "\n",
    "    print(\"\\n=== Test Metrics (Tuned) ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred_tuned, zero_division=0))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred_tuned, zero_division=0))\n",
    "    print(\"F1:\", f1_score(y_test, y_pred_tuned, zero_division=0))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, pred_proba_tuned))\n",
    "    print(\"PR-AUC:\", average_precision_score(y_test, pred_proba_tuned))\n",
    "\n",
    "    plt.figure()\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred_tuned)\n",
    "    plt.title(f\"Confusion Matrix — {model_to_tune} (Tuned)\")\n",
    "    plt.show()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, pred_proba_tuned)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=\"ROC (tuned)\")\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(f\"ROC Curve — {model_to_tune} (Tuned)\")\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "    prec, rec, _ = precision_recall_curve(y_test, pred_proba_tuned)\n",
    "    plt.figure()\n",
    "    plt.plot(rec, prec, label=\"PR (tuned)\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"Precision-Recall Curve — {model_to_tune} (Tuned)\")\n",
    "    plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048917c9",
   "metadata": {},
   "source": [
    "## 9) Final Comparison & Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbbdfe7",
   "metadata": {},
   "source": [
    "\n",
    "- CV table summarizes baseline models; we selected the best by ROC‑AUC and evaluated on a hold‑out test set.\n",
    "- RandomizedSearchCV refines hyperparameters for gains.\n",
    "- Scaling to full data (~1.5M rows):\n",
    "  - Consider increasing `n_iter` and `CV_FOLDS` if runtime allows.\n",
    "  - Add experiment tracking (MLflow/wandb).\n",
    "  - Persist models with `joblib` and track schema/version.\n",
    "  - Consider probability calibration and fairness audits across subgroups.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
